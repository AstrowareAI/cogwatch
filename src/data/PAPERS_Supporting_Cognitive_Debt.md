# **Paper 1 — METR (2025): Early-2025 AI & Experienced Dev Productivity (RCT)**

**Citation (link):** METR, “Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity,” **July 10, 2025**. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

**One-line finding :**

Allowing experienced open-source developers to use AI **made them slower by 19%** on average in this RCT. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

**Key numbers :**

* **Effect size:** Developers **took 19% longer** with AI vs without. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

* **Expectations vs reality:** Before tasks, developers **expected \+24% speed-up**; even after experiencing a slowdown, they **still believed \+20% speed-up**. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

* **Sample:** **16** experienced OSS developers; **246** real issues from their own large repos. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

* **Task context:** Issues averaged **\~2 hours**; repos averaged **22k+ stars** and **1M+ LOC**; participants paid **$150/hr**. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

* **AI setup:** “AI-allowed” condition used **Cursor Pro** with **Claude 3.5/3.7 Sonnet** (frontier models at the time). [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

* **Robustness:** Slowdown **persists across outcome measures**; **quality of PRs** was similar with/without AI; extensive **factor analysis** included. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

**Design & method :**

* **Randomized controlled trial**: Each issue randomly assigned to **AI-allowed** or **AI-disallowed** treatment. Developers worked on **their own** repos and issues (ecologically valid). [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

**Limitations :**

* Domain-specific (experienced OSS devs, high quality standards).

* Tooling & learning-curve caveats (Cursor sampling, prompting/scaffolding could change results).

* Authors explicitly **do not claim** this generalizes to all devs or domains. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

**Why this matters :**

* One of the few **clean RCTs** on real developer work; provides a **quantitative anchor (-19%)** that contrasts with self-reports and some benchmarks. [metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)

# **Paper 2 — MIT Media Lab: *Your Brain on ChatGPT: Accumulation of Cognitive Debt* (2025)**

**Citation (links):**

* arXiv preprint (June 10, 2025): [https://arxiv.org/abs/2506.08872](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com). [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

* Project/site: [https://www.brainonllm.com/](https://www.brainonllm.com/?utm_source=chatgpt.com) (overview). [brainonllm.com](https://www.brainonllm.com/?utm_source=chatgpt.com)

**One-line finding :**

In an essay-writing task, the **LLM group showed the weakest neural connectivity and lowest memory/ownership**, relative to “brain-only” and “search-engine” groups. [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

**Key numbers :**

* **Sample:** **54 participants** completed Sessions 1–3; **18** completed Session 4 (cross-over). [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

* **Conditions:** Three groups — **LLM**, **Search Engine**, **Brain-only** (no tools). Session 4 swaps: **LLM→Brain** and **Brain→LLM**. [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

* **Measurement:** EEG-based brain connectivity during writing; essay analysis with NLP \+ human teachers \+ AI judge. [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

* **Neural result:** **Brain-only \> Search \> LLM** in connectivity/engagement; “LLM-to-Brain” group later showed **reduced alpha/beta connectivity**, indicating under-engagement carryover. [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

* **Behavioural/memory:** **LLM users struggled to accurately quote their own work**; **self-reported ownership lowest** in LLM group. [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

**Design & method :**

* Multi-session **experimental** study of SAT-style essay writing with **between-group** conditions, EEG monitoring, and cross-over in Session 4; essays evaluated by human graders and an AI judge; linguistic structure compared via NERs/n-grams/ontology. [arxiv.org](https://arxiv.org/abs/2506.08872?utm_source=chatgpt.com)

**Limitations :**

* **Preprint (not yet peer-reviewed)**; modest **N**; educational essay-writing context may not generalize to other tasks/populations; EEG connectivity is a **proxy**, not a clinical diagnosis. [Le Monde.fr+1](https://www.lemonde.fr/en/science/article/2025/07/02/chatgpt-use-significantly-reduces-brain-activity-an-mit-study-finds_6742927_10.html?utm_source=chatgpt.com)

**Why this matters :**

* Provides **direct neural \+ behavioural evidence** consistent with a **“cognitive debt”** mechanism during sustained LLM-assisted writing; complements survey findings on reduced critical-thinking effort.

# **Paper 3 — Microsoft Research & CMU (2025): *The Impact of Generative AI on Critical Thinking***

**Citation (links):**

* PDF (CHI ’25): [https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee\_2025\_ai\_critical\_thinking\_survey.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf?utm_source=chatgpt.com). [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

* Project page: [https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/](https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/?utm_source=chatgpt.com) . [Microsoft](https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/?utm_source=chatgpt.com)

**One-line finding :**

Among 319 knowledge workers, **higher confidence in GenAI is associated with less critical thinking**, while **higher self-confidence is associated with more**. [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

**Key numbers :**

* **Sample:** **319** knowledge workers; **936** first-hand GenAI task examples. [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

* **Perceived effort ↓ with GenAI** (share reporting “less/much less effort”): **Knowledge 72%**, **Comprehension 79%**, **Application 69%**, **Analysis 72%**, **Synthesis 76%**, **Evaluation 55%**. [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

* **Model results (direction):** Higher **confidence in GenAI → less critical thinking** (and less perceived effort); higher **self-confidence → more critical thinking** (and more perceived effort). Examples of coefficients reported include **β \= −0.12 (p=0.026)** and **β \= −0.23 (p\<0.001)** in specific cognitive activities; trust in GenAI negatively correlated with perceived effort across 4 of 6 activities (e.g., **Knowledge β=−0.12, p=0.029; Application β=−0.17, p=0.002; Analysis β=−0.12, p=0.046; Evaluation β=−0.24, p\<0.001**). [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

**Design & method :**

* CHI ’25 paper; cross-sectional **survey \+ modelling** of **319** weekly GenAI users (e.g., ChatGPT, Copilot). Analyzed **936** task instances; linked **confidence/trust** factors to **critical-thinking enactment/effort** (Bloom’s taxonomy). [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

**Limitations :**

* Self-reported measures; English-speaking sample; **perceived effort** is a proxy (not objective cognition). Cross-sectional (not longitudinal). [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)

**Why this matters :**

* Quantifies a **directional cognitive-debt mechanism**: as users’ confidence/trust in GenAI rises, **they engage less in critical thinking**—consistent with the cognitive off-loading pattern we need to parameterize. [Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf)


# **Paper 4 — Stack Overflow Developer Survey 2025 (AI section)**

**Citation (link):** [https://survey.stackoverflow.co/2025/ai](https://survey.stackoverflow.co/2025/ai?utm_source=chatgpt.com) [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

**One-line finding :**

**Adoption is very high, trust is low**: 84% use or plan to use AI tools; 46% actively distrust their accuracy vs 33% who trust; only 3.1% “highly trust.” [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

**Key numbers :**

* **Adoption:** “**84%** of respondents are using or planning to use AI tools…; **51%** of professional developers use AI tools **daily**.” [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

* **All respondents usage detail:** Daily **47.1%**, Weekly **17.7%**, Monthly/infrequent **13.7%**, Plan soon **5.3%**, Don’t plan **16.2%**. (Responses: 33,662) [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

* **Trust in accuracy (All respondents):** **Highly trust 3.1%**, **Somewhat trust 29.6%**, **Somewhat distrust 26.1%**, **Highly distrust 19.6%**. (Trusters **33%** vs Distrusters **46%**) (Responses: 33,244) [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

* **Complex tasks:** Only **4.4%** say AI handles complex tasks “very well”; **22.0%** “bad,” **17.6%** “very poor,” **16.8%** don’t use/don’t know. (All respondents) [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

**Design & method :**

* Global annual survey; **49k+** total respondents; AI section reports **33k+** responses on relevant items. (See survey landing pages for totals.) [survey.stackoverflow.co+1](https://survey.stackoverflow.co/?utm_source=chatgpt.com)

**Limitations :**

* Self-reported survey (not an experiment); “distrust” and “complex” are perceptions, not objective cognition/quality measures. [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

**Why this matters :**

* Establishes **prevalence** of AI usage and **perceived cognitive friction** (distrust/complexity), which complements experimental results on productivity (METR) and cognition (MIT/Microsoft). [survey.stackoverflow.co](https://survey.stackoverflow.co/2025/ai)

# **Paper 5 — OpenAI (2025): *Strengthening ChatGPT’s responses in sensitive conversations* \+ *Addendum to GPT-5 System Card***

**Citation (links):**

* Blog post (Oct 27, 2025): [https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/?utm_source=chatgpt.com) [OpenAI](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

* Addendum PDF (Oct 27, 2025): [https://cdn.openai.com/pdf/3da476af-b937-47fb-9931-88a851620101/addendum-to-gpt-5-system-card-sensitive-conversations.pdf](https://cdn.openai.com/pdf/3da476af-b937-47fb-9931-88a851620101/addendum-to-gpt-5-system-card-sensitive-conversations.pdf?utm_source=chatgpt.com) [OpenAI](https://cdn.openai.com/pdf/3da476af-b937-47fb-9931-88a851620101/addendum-to-gpt-5-system-card-sensitive-conversations.pdf)

**One-line finding :**

OpenAI estimates that in a typical week **\~0.07% of users** show possible signs of **psychosis/mania** and **\~0.15% of users** show **explicit indicators of suicidal planning or intent** in conversations. [OpenAI](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

**Key numbers :**

* **Psychosis/mania signals (users):** **\~0.07% of weekly active users**; **messages:** **\~0.01%**. [OpenAI](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

* **Suicidal planning/intent (users):** **\~0.15% of weekly active users**; **messages:** **\~0.05%**. [OpenAI](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

* **Expert collaboration:** **170+** clinicians involved; improvements reduced undesired responses by **65–80%** across domains. [OpenAI](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

* **Eval improvements:** On challenging mental-health conversations, **\+39%** (vs GPT-4o) improvement; automated evals **\~92%** compliance (new model) vs **27%** (prior GPT-5) on mental-health set; **\~91%** vs **77%** on suicide/self-harm set. [OpenAI+1](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

**Design & method :**

* Internal production traffic estimates for **low-prevalence** events (psychosis/mania; suicide/self-harm; emotional reliance). Baseline \+ offline evals; clinician grading (**1,800+** model responses) reported in the addendum. [OpenAI+1](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

**Limitations :**

* OpenAI notes these are **initial estimates** for **rare** events; **overlap** across categories is possible; prevalence sensitive to taxonomy and detection methodology; **not clinical diagnoses**. [OpenAI](https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/)

**Why this matters :**

* Provides **explicit prevalence rates** for psychosis/mania and suicidal-intent signals in ChatGPT conversations—critical anchors for modelling **population-level mental-health risk** tied to AI usage. (If framing absolute counts, multiply by current **WAU**; e.g., with **\~800M WAU**, **0.07% ≈ 560k/week**, **0.15% ≈ 1.2M/week**.) [WIRED](https://www.wired.com/story/chatgpt-psychosis-and-self-harm-update/?utm_source=chatgpt.com)

Great—adding the extra paper, in the same copy-ready format.

---

# 

# **Paper 6 — HumanAgencyBench (2025): *Scalable Evaluation of Human Agency Support in AI Assistants***

**Citation (links):**

* arXiv (Sept 10, 2025): [https://arxiv.org/abs/2509.08494](https://arxiv.org/abs/2509.08494). ([arXiv](https://arxiv.org/abs/2509.08494))

* HTML/PDF mirror (for figures/tables): [https://ar5iv.org/html/2509.08494](https://ar5iv.org/html/2509.08494). ([ar5iv](https://ar5iv.org/pdf/2509.08494))

**One-line finding :**

Across **20 LLMs**, **human-agency support is low-to-moderate**, with large variation by developer and by dimension (e.g., clarify questions, encourage learning, defer decisions). ([ar5iv](https://ar5iv.org/pdf/2509.08494))

**Key numbers :**

* **Benchmark setup:** **6 dimensions**, **500 tests/dimension** → **3,000 tests/model**; also **468 human raters** on a subset (preregistered). ([ar5iv](https://ar5iv.org/pdf/2509.08494))

* **Defer Important Decisions:** overall **38.7% (±0.4%)**. By developer: **Anthropic 60.7% (±0.87%)**, **Google 42.5% (±0.9%)**, **Meta 30.3% (±1.0%)**, **xAI 21.4% (±1.7%)**, **OpenAI 21.2% (±0.6%)**. Model range (OpenAI): **o3 48.8% (±2.1%) → GPT-4.1 3.5% (±0.7%)**. ([ar5iv](https://ar5iv.org/pdf/2509.08494))

* **Encourage Learning:** overall **30.5% (±0.3%)**; best **Claude-3.5-Sonnet-20241022 48.3% (±1.6%)**; **xAI Grok-3 48.3% (±1.7%)**; most others **27.7–33.1%**. ([ar5iv](https://ar5iv.org/pdf/2509.08494))

* **Maintain Social Boundaries:** overall **37.2% (±0.4%)**; top scores \~**90%** (e.g., **Claude-3.5-Haiku-20241022 93.5% (±0.7%)**, **Claude-3.5-Sonnet-20240620 91.6% (±1.1%)**). ([ar5iv](https://ar5iv.org/pdf/2509.08494))

**Design & method :**

* **HumanAgencyBench (HAB)** evaluates whether assistants **Ask Clarifying Questions**, **Avoid Value Manipulation**, **Correct Misinformation**, **Defer Important Decisions**, **Encourage Learning**, **Maintain Social Boundaries**—using LLM-simulated queries and LLM-as-judge, then compared with **human ratings**. ([ar5iv](https://ar5iv.org/pdf/2509.08494))

**Limitations :**

* **Benchmark proof-of-concept**; “agency” is partly conceptual; LLM-as-judge; results vary by developer/model; not a longitudinal cognition study. ([ar5iv](https://ar5iv.org/pdf/2509.08494))

**Why this matters :**

* Agency-support dimensions (esp. **Encourage Learning** and **Defer Important Decisions**) are **directly relevant to “cognitive debt”**—they quantify how assistants **either scaffold or short-circuit user thinking/ownership** at scale. Low-to-moderate scores indicate **non-trivial risk of cognitive off-loading** in typical usage. ([ar5iv](https://ar5iv.org/pdf/2509.08494))

